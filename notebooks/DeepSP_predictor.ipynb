{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqYhZfJnSvsH"
   },
   "source": [
    "Install and Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "icCMgpaXYyFN"
   },
   "outputs": [],
   "source": [
    "# Environment setup for Colab/notebook environment\n",
    "!pip install -q condacolab\n",
    "import condacolab\n",
    "condacolab.install() # Installs anaci via conda below\n",
    "\n",
    "!conda install -c bioconda anarci --yes\n",
    "\n",
    "# Install specific package versions (consider if these are strictly necessary or if latest compatible versions are okay)\n",
    "!pip install keras==2.11.0 tensorflow==2.11.0 scikit-learn==1.0.2 pandas numpy biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOqM10kcxHb9"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "# Import machine learning libraries (Keras is part of TensorFlow now)\n",
    "from tensorflow.keras.models import model_from_json \n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n",
    "# Attempt to import utils from scripts directory\n",
    "# This requires the notebook to be in a directory where 'scripts' is a sibling or accessible via sys.path\n",
    "# Assuming the notebook is in 'notebooks' and 'scripts' is a sibling directory\n",
    "module_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'scripts'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    print(f\"Appended to sys.path: {module_path}\")\n",
    "else:\n",
    "    print(f\"{module_path} already in sys.path\")\n",
    "\n",
    "try:\n",
    "    from utils import (\n",
    "        one_hot_encoder,\n",
    "        parse_anarci_results_to_aligned_sequences,\n",
    "        predict_properties,\n",
    "        get_project_root,\n",
    "        dataframe_to_fasta_string\n",
    "    )\n",
    "    print(\"Successfully imported from utils.py\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing from utils.py: {e}\")\n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "    print(f\"Sys.path: {sys.path}\")\n",
    "    print(\"Please ensure utils.py is in the ../scripts directory relative to the notebook, or adjust sys.path.\")\n",
    "    # Define fallback functions if utils.py is not found (for basic execution, not ideal)\n",
    "    def get_project_root(): return Path('.').resolve().parent # Basic fallback\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nifzJMA-TkHp"
   },
   "source": [
    "Define Project Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "notebook-paths-setup"
   },
   "outputs": [],
   "source": [
    "PROJECT_ROOT = get_project_root() \n",
    "# For notebooks, utils.py is in ../scripts. So get_project_root() from utils.py will correctly point to the parent of scripts.\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "INPUT_CSV_PATH = DATA_DIR / \"input\" / \"DeepSP_input.csv\"\n",
    "OUTPUT_CSV_PATH = DATA_DIR / \"DeepSP_descriptors_notebook.csv\" # Different name to avoid conflict\n",
    "\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"Data Directory: {DATA_DIR}\")\n",
    "print(f\"Input CSV: {INPUT_CSV_PATH}\")\n",
    "print(f\"Output CSV: {OUTPUT_CSV_PATH}\")\n",
    "\n",
    "# Temporary directory for ANARCI files within the notebook's execution environment\n",
    "TEMP_ANARCI_DIR = Path('.') / \"temp_anarci_notebook_processing\" # Create in current notebook directory\n",
    "TEMP_ANARCI_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "HEAVY_FASTA_PATH = TEMP_ANARCI_DIR / \"seq_H.fasta\"\n",
    "LIGHT_FASTA_PATH = TEMP_ANARCI_DIR / \"seq_L.fasta\"\n",
    "ANARCI_H_CSV_PATH = TEMP_ANARCI_DIR / \"seq_aligned_H.csv\"\n",
    "ANARCI_L_CSV_PATH = TEMP_ANARCI_DIR / \"seq_aligned_KL.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import-dataset-markdown"
   },
   "source": [
    "Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "id": "gp2-sVDPSTXh"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    dataset = pd.read_csv(INPUT_CSV_PATH)\n",
    "    print(f\"Successfully loaded dataset from {INPUT_CSV_PATH}\")\n",
    "    display(dataset.head()) # In Jupyter, use display() for rich DataFrame output\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Input CSV file not found at {INPUT_CSV_PATH}\")\n",
    "    # Stop execution or handle error appropriately for a notebook\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-qttNLlTuT4"
   },
   "source": [
    "Convert DataFrame to Fasta Files for ANARCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pt5KeAfZy8gF"
   },
   "outputs": [],
   "source": [
    "with open(HEAVY_FASTA_PATH, \"w\") as h_out, open(LIGHT_FASTA_PATH, \"w\") as l_out:\n",
    "    for _, row in dataset.iterrows():\n",
    "        h_out.write(f\">{row['Name']}\\n{row['Heavy_Chain']}\\n\")\n",
    "        l_out.write(f\">{row['Name']}\\n{row['Light_Chain']}\\n\")\n",
    "print(f\"Created FASTA files: {HEAVY_FASTA_PATH}, {LIGHT_FASTA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QugBcnYeT1ci"
   },
   "source": [
    "Sequence Alignment with ANARCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h7DCE-fo16qG"
   },
   "outputs": [],
   "source": [
    "# Using subprocess for ANARCI calls, similar to the refactored deepsp_predictor.py\n",
    "# The -o for ANARCI specifies a prefix, ANARCI adds _H.csv or _KL.csv itself.\n",
    "anarci_output_prefix_H = TEMP_ANARCI_DIR / \"seq_aligned\"\n",
    "anarci_output_prefix_L = TEMP_ANARCI_DIR / \"seq_aligned\"\n",
    "\n",
    "anarci_cmd_H = [\n",
    "    \"ANARCI\", \"-i\", str(HEAVY_FASTA_PATH), \"-o\", str(anarci_output_prefix_H),\n",
    "    \"-s\", \"imgt\", \"-r\", \"heavy\", \"--csv\"\n",
    "]\n",
    "anarci_cmd_L = [\n",
    "    \"ANARCI\", \"-i\", str(LIGHT_FASTA_PATH), \"-o\", str(anarci_output_prefix_L),\n",
    "    \"-s\", \"imgt\", \"-r\", \"light\", \"--csv\"\n",
    "]\n",
    "\n",
    "print(f\"Running ANARCI for Heavy Chains: {' '.join(anarci_cmd_H)}\")\n",
    "process_H = subprocess.run(anarci_cmd_H, capture_output=True, text=True, check=False)\n",
    "if process_H.returncode != 0:\n",
    "    print(f\"ANARCI Error (Heavy Chains):\\n{process_H.stderr}\")\n",
    "else:\n",
    "    print(\"ANARCI Heavy Chain processing successful.\")\n",
    "    # print(process_H.stdout) # For debugging\n",
    "\n",
    "print(f\"Running ANARCI for Light Chains: {' '.join(anarci_cmd_L)}\")\n",
    "process_L = subprocess.run(anarci_cmd_L, capture_output=True, text=True, check=False)\n",
    "if process_L.returncode != 0:\n",
    "    print(f\"ANARCI Error (Light Chains):\\n{process_L.stderr}\")\n",
    "else:\n",
    "    print(\"ANARCI Light Chain processing successful.\")\n",
    "    # print(process_L.stdout) # For debugging\n",
    "\n",
    "# Verify output files\n",
    "if not ANARCI_H_CSV_PATH.exists() or not ANARCI_L_CSV_PATH.exists():\n",
    "    print(f\"Error: ANARCI did not produce the expected output CSV files at {TEMP_ANARCI_DIR}\")\n",
    "    print(f\"Checked for: {ANARCI_H_CSV_PATH}, {ANARCI_L_CSV_PATH}\")\n",
    "    # ANARCI might create files in the current dir if -o path is problematic, check there.\n",
    "    # This was a fallback in predictor, might be needed here too.\n",
    "    if Path(\"seq_aligned_H.csv\").exists() and Path(\"seq_aligned_KL.csv\").exists():\n",
    "        print(\"Found ANARCI output in current directory. Moving them.\")\n",
    "        shutil.move(\"seq_aligned_H.csv\", ANARCI_H_CSV_PATH)\n",
    "        shutil.move(\"seq_aligned_KL.csv\", ANARCI_L_CSV_PATH)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"ANARCI output files not found. Searched in {TEMP_ANARCI_DIR} and current directory.\")\n",
    "else:\n",
    "    print(f\"ANARCI output CSVs found: {ANARCI_H_CSV_PATH}, {ANARCI_L_CSV_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "parse-anarci-results-markdown"
   },
   "source": [
    "Parse ANARCI Results and Align Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vn4-q554Udy_"
   },
   "outputs": [],
   "source": [
    "# The seq_preprocessing function is now replaced by utils.parse_anarci_results_to_aligned_sequences\n",
    "# The constants (H_inclusion_list, L_inclusion_list, H_dict, L_dict) are in utils.py\n",
    "try:\n",
    "    valid_name_list, aligned_seq_list = parse_anarci_results_to_aligned_sequences(\n",
    "        str(ANARCI_H_CSV_PATH), str(ANARCI_L_CSV_PATH)\n",
    "    )\n",
    "    if not aligned_seq_list:\n",
    "        print(\"Warning: No sequences were successfully aligned or parsed from ANARCI results.\")\n",
    "    else:\n",
    "        print(f\"Successfully parsed {len(aligned_seq_list)} aligned sequences.\")\n",
    "        # print(\"First few aligned sequences:\", aligned_seq_list[:2]) # For debugging\n",
    "except Exception as e:\n",
    "    print(f\"Error during ANARCI result parsing: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0SoIZ19Un54"
   },
   "source": [
    "One Hot Encoding of Aligned Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8maPu9TsUnU0"
   },
   "outputs": [],
   "source": [
    "# The load_input_data function is no longer needed as we have aligned_seq_list\n",
    "if not aligned_seq_list: # Check if list is empty\n",
    "    print(\"Aligned sequence list is empty. Skipping one-hot encoding.\")\n",
    "    X_processed = np.array([])\n",
    "else:\n",
    "    X_one_hot = [one_hot_encoder(s=x) for x in aligned_seq_list] # one_hot_encoder from utils.py\n",
    "    X_processed = np.transpose(np.asarray(X_one_hot), (0, 2, 1))\n",
    "    X_processed = np.asarray(X_processed)\n",
    "    print(f\"Processed data shape for model input: {X_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QrdyGSKQWv0V"
   },
   "source": [
    "Predict DeepSP Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Xyrqu5dXwxq"
   },
   "outputs": [],
   "source": [
    "if X_processed.size == 0:\n",
    "    print(\"Skipping prediction as there is no processed data.\")\n",
    "    df_predictions = pd.DataFrame()\n",
    "else:\n",
    "    # predict_properties function from utils.py handles loading models and predicting\n",
    "    # It uses get_project_root() / \"data\" as the default model_base_path if None is passed\n",
    "    df_predictions = predict_properties(X_processed, model_base_path=str(DATA_DIR))\n",
    "    print(\"Predictions complete.\")\n",
    "    if not df_predictions.empty:\n",
    "        display(df_predictions.head())\n",
    "    else:\n",
    "        print(\"Prediction resulted in an empty DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save-results-markdown"
   },
   "source": [
    "Save Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "id": "a-FfSETHegbu"
   },
   "outputs": [],
   "source": [
    "if not df_predictions.empty and valid_name_list:\n",
    "    df_name = pd.DataFrame(valid_name_list, columns=[\"Name\"]) # Use names of successfully processed sequences\n",
    "    df_final = pd.concat([df_name.reset_index(drop=True), df_predictions.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    df_final.to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "    print(f\"Successfully saved DeepSP descriptors to {OUTPUT_CSV_PATH}\")\n",
    "    display(df_final.head())\n",
    "else:\n",
    "    print(\"No predictions to save, or name list is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup-markdown"
   },
   "source": [
    "Clean up Temporary Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R0Gi5_po05Ct"
   },
   "outputs": [],
   "source": [
    "if TEMP_ANARCI_DIR.exists():\n",
    "    try:\n",
    "        shutil.rmtree(TEMP_ANARCI_DIR)\n",
    "        print(f\"Successfully removed temporary directory: {TEMP_ANARCI_DIR}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error removing temporary directory {TEMP_ANARCI_DIR}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
